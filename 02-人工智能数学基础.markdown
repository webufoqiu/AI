## 人工智能的数学基础

​		

### 一、函数

#### 一次函数：

##### 一个自变量的情形：

​							$$y=ax+b  $$   

##### 多个自变量的情形：

​							$$ y=ax_1+bx_2+c (a、b、c是常数) $$

##### 神经网络中的一次函数：

​		神经网络中，神经单元的加权输入可以表示为一次函数关系。例如，神经单元有三个来自下层的输入，其加权输入 *z* 的式子如下所示：

​							$$z=w_1x_1+w_2x_2+w_3x_3+b$$

- 如果把作为参数的权重 $w_1,w_2,w_3$与偏置 $b$ 看作常数，那么加权输入 $z$ 和$x_1,x_2,x_3$是一次函数关系。
- 另外，在神经单元的输入 $x_1,x_2,x_3$作为数据值确定了的情况下，加权输入$z$ 和权重 $w_1,w_2,w_3$以及偏置$b$是一次函数关系。
- 用误差反向传播法推导计算式时，这些一次函数关系使得计算可以简单地进行。



#### 二次函数：

##### 机器学习和深度学习中的代价函数使用了二次函数。

二次函数由下式表示：  $y=ax^2+bx+c$

![image-20210527095101484](深度学习.assets/image-20210527095101484-1622080266513.png)这个图像中重要的一点是，$a$ 为正数时图像向下凸，从而存在最小值。

下凸的函数有最小值，这个性质是机器学习中最小二乘法（损失函数的平方误差总和）的基础

##### 多个自变量的二次函数：

$$y=ax_1^2+bx_1x_2+cx_2^2+px_1+qx_2+r$$

![image-20210527100116577](深度学习.assets/image-20210527100116577-1622080879219.png)神经网络中要处理的二次函数



#### 单位阶跃函数：

神经网络的原型模型是用单位阶跃函数作为激活函数

$$f(x)=\begin{cases}0, & \text{(x<  0)} \\1, & \text{(x }\geq\text{0)}\end{cases}$$ 此函数在原点不可导，所以不能作为主要的激活函数

![image-20210527104504044](深度学习.assets/image-20210527104504044.png)

#### 指数函数与Sigmoid函数：

##### 指数函数：$$y=a^x     , (a>0,\text{a}\ne\text{1})$$

##### 重要的底：自然数，欧拉数，纳皮尔数 $e=2.71828 \cdots$

##### Sigmoid函数：

$$\sigma(x)=\frac{1}{1+e^{-x}}$$     $\sigma(x)$是神经网络中，代表性的激活函数

<img src="深度学习.assets/image-20210527110037925-1622084440838.png" alt="image-20210527110037925" style="zoom: 67%;" />



#### 正态分布的概率密度函数

用计算机实际确定神经网络时，必须设定权重和偏置的初始值。求初始值时，正态分布（normal

 distribution）是一个有用的工具。使用服从这个分布的随机数，容易取得好的结果。

正态分布是服从以下概率密度函数 *f*(*x*) 的概率分布

$$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^\frac{(x-\mu)^2}{2\sigma^2}$$    其中$\sigma$是标准差，$\mu$ 是期望值（平均值）

<img src="深度学习.assets/image-20210527111046300-1622085048056.png" alt="image-20210527111046300" style="zoom:80%;" />

<img src="深度学习.assets/image-20210527111158868-1622085121545.png" alt="image-20210527111158868" style="zoom:80%;" />

在 Excel 中，可以像下面这样产生正态分布随机数。

$= NORM.INV(RAND(), µ, σ)    \qquad \qquad（µ、σ 是期望值和标准差）$

<img src="深度学习.assets/image-20210527122626831-1622089589464.png" alt="image-20210527122626831" style="zoom:67%;" />



**方差或者说标准差越大，离散程度越大**

参考阅读：https://blog.csdn.net/zhaodedong/article/details/103775963 （关于期望值和平均值）

参考阅读：https://blog.csdn.net/u013066730/article/details/83029619  （方差和标准差）





### 二、数列



理解了数列的递推公式，就很容易理解：求解预估值的正向传播（链式求值）、误差的反向传播（链式求导）

注意：计算机更擅长用递推公式进行运算，联想下程序设计中的递归

#### 数列的通项公式和递推公式

- 等差数列通项公式：$a_n=a_1+(n-1)d$
- 等比数列通项公式：$a_n=a_1q^{n-1}$
- 等差数列递推公式：$a_{n+1}=a_n+d$
- 等比数列递推公式：$a_{n+1}=a_nq$
- 神经网络中，每一个神经单元可以理解为数列中的某一项

#### 联立的递推公式

$$\begin{cases}a_{n+1}=a_n+2b_n+2 \\ b_{n+1}=2a_n+3b_n+3 \end{cases}$$

如上，两个数列的联立递推公式，尝试理解如下神经网络中每个神经单元的值的计算



<img src="深度学习.assets/image-20210527130632915-1622091995702.png" alt="image-20210527130632915" style="zoom:80%;" />

**注：其中，$a_n^m ,b_n^m$    ：第m层第n个神经单元的值和偏置（激活神经单元的阈值） ；**

**注：其中，$w_i^j$ ：  第j层每个结点接收上层对应结点的数据的权重**



$$\begin{cases} a_1^3=\alpha(w_{11}^3a_1^2+w_{12}^3a_2^2+w_{13}^3a_3^2+b_1^3)\\  a_2^3=\alpha(w_{21}^3a_1^2+w_{22}^3a_2^2+w_{23}^3a_3^2+b_2^3) \end{cases}$$



根据这个递推公式，第 3 层的输出 $a_1^3$和$a_2^3$由第 2 层的输出 $a_1^2$、 $a_2^2$ 、$a_3^2$决定。

也就是说，第 2 层的输出与第 3 层的输出由联立递推公式联系起来。



### 三、∑符号的理解和使用

∑是一个需要下功夫来熟悉的符号。如果不理解∑，在阅读神经网络相关的文献时就比较麻烦。

这是因为将加权输入用∑符号来表示会简洁得多。

#### 常见的希腊字母：

| \sigma   | $$\Sigma \text{大写}   \qquad \sigma  \text{小写}$$    |
| -------- | ------------------------------------------------------ |
| \alpha   | $$\Alpha \text{大写}   \qquad \alpha  \text{小写}$$    |
| \beta    | $$\Beta \text{大写}   \qquad \beta \text{小写}$$       |
| \gamma   | $$\Gamma \text{大写}   \qquad \gamma  \text{小写}$$    |
| \delta   | $$\Delta \text{大写}   \qquad \delta \text{小写}$$     |
| \epsilon | $$\Epsilon \text{大写}   \qquad \epsilon \text{小写}$$ |
| \zeta    | $$ \Zeta \text{大写}   \qquad \zeta\text{小写}$$       |
| \eta     | $$\Eta \text{大写}   \qquad \eta \text{小写}$$         |
| \theta   | $$\Theta \text{大写}   \qquad \theta  \text{小写}$$    |
| \lambda  | $$\Lambda \text{大写}   \qquad \lambda  \text{小写}$$  |

#### ∑符号含义：表示数列的总和

$$\sum_{i=1}^n a_i= a_1+a_2+\cdots+a_n$$

#### ∑符号性质：和的∑为∑的和，常数倍的∑为∑的常数倍

$$\sum_{i=1}^n(a_i+b_i)=\sum_{i=1}^na_i+\sum_{i=1}^nb_i$$

$$\sum_{i=1}^nca_i=c\sum_{i=1}^na_i$$

**在机器学习和深度学习的文献中，把加权输入用∑符号不是非常简洁**。



### 四、向量(Vector)

#### 定义：

- 有向线段的属性：起点 *A* 的位置、指向 *B* 的方向，以及*AB* 的长度，也就是大小
- 把方向与大小抽象出来，这样的量叫作**向量**
- 向量是具有方向与大小的量，用箭头表示。
- 有向线段 *AB* 所代表的向量用 $\vec{AB}$ 表示，也可以用带箭头的单个字母$\vec{a}$表示。



#### 向量的坐标表示法

- 把向量的箭头放在坐标系中，就可以用坐标的形式表示向量。
- 把箭头的起点放在原点，用箭头终点的坐标表示向量。
- 用坐标表示的向量 $\vec{a}$，$\vec{a}=(a_1,a_2)$
- 三维空间同理：$\vec{a}=(a_1,a_2,a_3)$  ，$\vec{a}=(1,2,2)$，如图

<img src="深度学习.assets/image-20210527150751221.png" alt="image-20210527150751221" style="zoom:80%;" />

#### 向量的大小： | **a** |

表示向量的箭头的**长度**称为这个向量的大小。向量 $\vec{a}$ 的大小用 | **a** | 表示。

#### 向量的内积：$$\vec{a}\cdot\vec{b} = \left| \vec{a} \right|\left| \vec{b}\right|\cos(\theta) $$

<img src="深度学习.assets/image-20210527152613424-1622100375353.png" alt="image-20210527152613424" style="zoom:67%;" />

#### 柯西 - 施瓦茨不等式：$-\left|a\right|\left|b\right|\le a \cdot  b \le \left|a\right|\left|b\right|$

根据柯西 - 施瓦茨不等式，可以得出以下事实。

- 当两个向量方向相反时，内积取得最小值。**梯度下降法的基本原理**
- 当两个向量不平行时，内积取平行时的中间值。**越大越相似，考察卷积神经网络**
- 当两个向量方向相同时，内积取得最大值

<img src="深度学习.assets/image-20210527154146908-1622101308932.png" alt="image-20210527154146908" style="zoom:67%;" />

#### 内积的坐标表示

当$\vec{a}=(a1,a2)$，$\vec{b}=(b1,b2)$时，

​		$$\vec{a} \cdot \vec{b}= a_1b_1+a_2b_2	$$

同理，三维空间情况下：当$\vec{a}=(a1,a2,a3)$，$$\vec{b}=(b1,b2,b3)$$时，

​		$$\vec{a} \cdot \vec{b}= a_1b_1+a_2b_2+a_3b_3$$

#### 向量的一般化

- 向量的方便之处在于，二维以及三维空间中的性质可以照搬到任意维空间中；
- 神经网络虽然要处理数万维的空间，但是二维以及三维空间的向量性质可以直接利用。基于此，向量被充分应用在梯度下降法中；
- 二维以及三维空间中的向量公式推广到任意的 *n* 维空间：
  - 向量的坐标表示法： $\vec{a}=(a_1,a_2,\cdots,a_n)$
  - 内积的坐标表示：$$\vec{a} \cdot \vec{b}= a_1b_1+a_2b_2+\cdots+a_nb_n$$

- 在神经网络中使用内积形式：
  - <img src="深度学习.assets/image-20210527161141507-1622103103036.png" alt="image-20210527161141507" style="zoom:67%;" />
  - 如图有两个向量：$\vec{x}=(x_1,x_2,\cdots,x_n)$  和 $\vec{w}=(w_1,w_2,\cdots,w_n)$
  - 加权输入可表示为向量内积形式：$z=\vec{x}\cdot\vec{w}+b$



### 五、矩阵(Matrix)

#### 什么是矩阵

- 矩阵就是数的阵列，如下：横排为行，竖排为列，构成一个3行3列的矩阵
- 行数和列数相同，称为**方阵**；
- (1,2,3)、(4,5,6)、(7,8,9)，称为**行向量**；(1,4,7)、(2,5,8)、(3,6,9)称为**列向量**；
- 行向量、列向量，也统称**向量**

$$
\begin{Bmatrix}
   1 & 2 & 3 \\
   4 & 5 & 6 \\
   7 & 8 & 9
  \end{Bmatrix}
$$

- 扩展为一般形式：m行n列的矩阵

$$
\left\{
 \begin{matrix}
 a_{11}      & a_{12}        & \cdots & a_{1n}        \\
 a_{21}      & a_{22}        & \cdots & a_{2n}       \\
 \vdots & \vdots   & \ddots & \vdots   \\
 a_{m1} & a_{m2} & \cdots & a_{mn} 
 \end{matrix}
 \right\}  \tag{m,n的矩阵}
$$

- 位于第 i 行第 j 列的值（称为元素），用$a_{ij}$表示

- 很有名的矩阵（**单位矩阵**）：对角线的元素$(a_{ii})$为1，其他元素为0的**方阵**，用**E**表示

  $$E=\begin{Bmatrix}
     1 &  0 & 0 \\
      0 & 1 &  0 \\
      0 &  0 & 1
    \end{Bmatrix} \tag{注：E 为德语中表示 1 的单词 Ein 的首字母}$$

#### 矩阵相等

​		两个矩阵 **A**、**B** 相等的含义是它们对应的元素相等，记为 **A** *=* **B**

#### 矩阵的和、差、常数倍

​		两个矩阵 **A**、**B** 的和 **A** *+* **B**、差 **A** - **B** 定义为相同位置的元素的和、差所产生的矩阵。此外，矩阵的常数倍定义为各个元素的常数倍所产生的矩阵。

#### 矩阵的乘积

- 矩阵的乘积在神经网络的应用中特别重要。对于两个矩阵 **A**、**B**，将**A** 的第 *i* 行看作行向量，**B** 的第 *j* 列看作列向量，将它们的**内积**作为第 *i*行第 *j* 列元素，由此而产生的矩阵就是矩阵 **A**、**B** 的乘积 $$A \cdot B$$。
- <img src="深度学习.assets/image-20210527204317388-1622119398950.png" alt="image-20210527204317388" style="zoom:80%;" />
- 矩阵的乘法不满足交换律。也就是说，除了例外情况，以下关系式成立：$$A \cdot B\ne B \cdot A$$
- 不是任何两个矩阵都能够相乘，**只有乘数矩阵A的列数和被乘矩阵B的行数相同的时候，两个矩阵才能相乘。**
- 维度为（m x n）的矩阵 A 和维度为（n x p）的矩阵 B 相乘，最终得到维度为（m x p）的矩阵 C。
- <img src="深度学习.assets/image-20210527205751320-1622120272827.png" alt="image-20210527205751320" style="zoom:67%;" />

#### Hadamard 乘积

对于相同形状的矩阵 **A**、**B**，将相同位置的元素相乘，由此产生的矩阵称为矩阵 **A**、**B** 的 **Hadamard** 乘积，用 **A**⊙**B** 表示。

#### 转置矩阵

- 将矩阵 **A** 的第 *i* 行第 *j* 列的元素与第 *j* 行第 *i* 列的元素交换，由此产生的矩阵称为矩阵 **A** 的转置矩阵（Transposed Matrix），用 $ t^A $ 或   $A^T$等表示。
- 可以理解为把行向量转换成列向量（反之亦然，把列向量转换成行向量）

<img src="深度学习.assets/image-20210527210238336-1622120559686.png" alt="image-20210527210238336" style="zoom:67%;" />



#### 标量（Scalar)和张量（Tensor）理解

- 在机器学习和深度学习中，核心的数据结构就是标量、向量、矩阵和张量；
- 其中标量是单个数字，或者说，若 x∈ℝ 表示 x 是一个标量。
- 其中张量是更泛化的实体，是对标量，向量和矩阵的更高层封装，可以理解为，张量的特例是矩阵，矩阵的特例是向量，向量的特例是标量；
- 如下图：多个矩阵，或者说6个矩阵组成的一个张量

<img src="深度学习.assets/image-20210527212438222-1622121879660.png" alt="image-20210527212438222" style="zoom:50%;" />

谷歌提供的人工智能学习系统（TensorFlow)的命名中使用了这个属于，据说来源于物理学中的"tension"(张力)，好比一个物体不同的截面上产生的应力方向和大小都不相同，张量是应力在数学上的抽象。





### 六、导数（derivatives）

#### 导数的定义

$$
f′(x) =\lim_{\Delta x\rightarrow 0}\frac{f(x+ \Delta x)-f(x)}{\Delta x}
$$

导函数的含义如下图所示。作出函数 *f* (*x*) 的图像，*f '*(*x*) 表示图像切线的斜率。因此，具有光滑图像的函数是可导的。

![image-20210527215447322](深度学习.assets/image-20210527215447322.png)

导函数的含义。$f'(x)$ 表示图像切线的斜率。实际上，如果 *Q* 无限接近 *P*（也就是∆*x* → 0），那么直线 *PQ* 无限接近切线 *l*。

#### 神经网络中用到的函数的导数公式

我们很少使用上面的定义式来求导函数，而是使用导数公式。

下面我们就来看一下在神经网络的计算中使用的函数的导数公式（*x* 为变量、*c* 为常数）。


$$
(c)'=0 \qquad\qquad  (e)'=0   \tag{常数的导数为0}\\
(x)'=1 \\
(x^2)'= 2x \\
(e^x)' = e^x \\
(e^{-x})' = -e^{-x}
$$

#### 导数符号

函数 *y* = *f* (*x*) 的导函数用 *f '*(*x*) 表示，但也存在不同的表示方法，比如可以用分数形式来表示：$$f'(x)=\frac{dy}{dx}   $$

这个表示方法是十分方便的，这是因为复杂的函数可以像分数一样计算导数



#### 导数的性质

线性性：和的导数为导数的和，常数倍的导数为导数的常数倍。
$$
[f(x)+g(x)]'=f'(x)+g'(x) \\
[cf(x)]' = cf'(x)
$$
导数的线性性是误差反向传播法背后的主角（损失函数loss的反向传播）



#### 分数函数的导数和 Sigmoid 函数的导数

当函数是分数形式时，求导时可以使用下面的分数函数的求导公式：
$$
[\frac{1}{f(x)}]'=-\frac{f(x)}{[f(x)]^2}
$$
Sigmoid 函数 *σ*(*x*) 是神经网络中最有名的激活函数之一：$\sigma(x)=\frac{1}{1+e^{-x}}$

将  $1+e^{-x}$ 代入上式：
$$
\sigma '(x)=- \frac{(1+e^{-x})'}{(1+e^{-x})^2}=\frac{e^{-x}}{(1+e^{-x})^2} =\frac{1+e^{-x}-1}{(1+e^{-x})^2}=\frac{1}{1+e^{-x}}-\frac{1}{(1+e^{-x})^2}
=\sigma(x)-\sigma(x)^2
$$

**提取$\sigma(x)$ ，得到： $ \color{red}\sigma '(x)= \sigma(x)(1-\sigma(x))$** 

梯度下降法中，需要对这个Sigmoid函数求导。求导时使用上式会十分方便。

划重点：Sigmoid函数求导 ：$ \color{red}\sigma '(x)= \sigma(x)(1-\sigma(x))$

#### 最小值的条件

由于导函数 *f'*(*x*) 表示切线斜率，我们可以得到以下原理：**当函数*f*(*x*)在*x* = *a*处取得最小值时，*f'*(*a*) = 0**。 

<img src="深度学习.assets/image-20210527225333876-1622127215402.png" alt="image-20210527225333876"  />

或者换句话说：***f'*(*a*) = 0 是函数 *f* (*x*)  在*x* = *a*   处取得最小值的必要条件**

**但是**，看看如下例子：

![image-20210527225701451](深度学习.assets/image-20210527225701451-1622127422982.png)

此函数，虽然是连续光滑的，但有不止一处增减性的变化，所以，**f'(a) = 0 是函数 *f* (*x*)  在*x* = *a*   处取得最小值的必要条件，仅仅是必要条件**

总结：需要连续光滑的**可导函数**，还要求是**凸函数**







### 七、神经网络的偏导数基础

神经网络的计算往往会涉及成千上万个变量，这是因为构成神经网络的神经单元的权重和偏置都被作为变量处理。现在讨论下神经网络计算中所需的**多变量函数**的导数

#### 多变量函数

有两个以上的自变量的函数称为多变量函数。如：

$z=x^2+y^2 \tag{x和y都是自变量}\\ 也可写成：f(x,y)=x^2+y^2$     

<img src="深度学习.assets/image-20210528090650601-1622164013313.png" alt="image-20210528090650601" style="zoom:67%;" />

再如：函数$f(x_1,x_2,\cdots,x_n)$：是有 *n* 个自变量$x_1,x_2,\cdots,x_n$的函数

#### 偏导数（partial derivative）

求导的方法也同样适用于多变量函数的情况。但是，由于有多个变量，所以必须指明对哪一个变量进行求导。在这个意义上，关于某个特定变量的导数就称为偏导数

对$f(x,y)=x^2+y^2 $ 这个多自变量函数，求偏导：

只看变量 x， 将 y 看作常数来求导：$\frac{\partial f(x,y)}{\partial x}$

只看变量 y， 将 x 看作常数来求导：$\frac{\partial f(x,y)}{\partial y}$

当 $z=f(w,x,b)=wx+b$  时，分别求偏导，得到：$\frac{\partial z}{\partial x} =w \qquad \frac{\partial z}{\partial w} =x \qquad  \frac{\partial z}{\partial b} =1 $

问：当$z=w_1x_1+w_2x_2+b$ 时，求关于$x1, w2, b $的偏导数



#### 多变量函数的最小值条件

- 光滑的单变量函数 *y* = *f* (*x*) 在点 *x* 处取得最小值的**必要条件**是：导函数在该点取值 0；

- 对于多变量函数，同样适用

- 函数$z=f(x,y)$取得最小值的必要条件是：$\frac{\partial f(x,y)}{\partial x}=0 \qquad \frac{\partial f(x,y)}{\partial y}=0 $

- 而且可以扩展到n个变量的多变量函数$f(x_1,x_2,\cdots,x_n)$

- 举例： 求函数$z=f(x,y)=x^2+y^2$取得最小值时 *x*、*y* 的值

  - 首先求关于x和关于y的偏导数：$\frac{\partial z}{\partial x}=2x \qquad \frac{\partial z}{\partial y}=2y$
  - 函数取得最小值的必要条件是 *x* = 0，*y* = 0。此时函数值 *z* 为 0。由于$x^2+y^2\ge0$，所以我们知道这个函数值 0 就是最小值。
  - 思考下：葡萄酒杯的底部~~

  <img src="深度学习.assets/image-20210528090650601-1622164013313.png" alt="image-20210528090650601" style="zoom:67%;" />



### 八、复合函数链式求导法则（误差反向传播法）

复杂（复合）函数求导的链式法则。这个法则对于理神经网络中误差反向传播法很有必要

#### 复合函数

已知函数  $y=f(u)$，当 *u* 表示为  $u=g(x)$  时，*y* 作为 *x* 的函数可以表示为形如  $y=f(g(x))$ 的嵌套结构（*u* 和 *x* 表示多变量）。这时，嵌套结构的函数 *f*(*g*(*x*)) 称为 *f*(*u*) 和 *g*(*x*) 的**复合函数**。

神经网络中，多个输入$x_1,x_2,\cdots,x_n$，将 *a*(*x*) 作为激活函数，求神经单元的输出 *y* 的过程如下：
$$
y=\alpha(w_1x_1+w_2x_2+\cdots+w_nx_n+b) \\
w_1,w_2,\cdots ,w_n 为各个输入的权重 \\
b为神经单元的偏置\\
y即为：f(x_1,x_2,\cdots,x_n)多变量一次函数 和 \alpha(x) 激活函数的复合函数
$$

#### 单变量函数的链式法则

单变量函数 $y=f(u)$，当 *u* 表示为单变量函数 $u=g(x)$时，复合函数 *f*(*g*(*x*)) 的导函数：
$$
\frac{dy}{dx}=\frac{dy}{du}\frac{du}{dx}
$$
同理：当 $y$为 $u$ 的函数，$u$为 $v$ 的函数，$v$为 $x$ 的单变量函数时：
$$
\frac{dy}{dx}=\frac{dy}{du}\frac{du}{dv}\frac{dv}{dx}
$$
举例：

有Sigmoid函数和线性函数（w,b是常数，所以$u$是**单变量函数**) ：
$$
y=\frac{1}{1+e^{-u}}\qquad，\qquad u=wx+b \\y=f(g(x))=\sigma(wx+b)
$$
求: $\frac{dy}{dx}$  ?

有：Sigmoid函数的导数公式：$\color{red}\sigma '(x)= \sigma(x)(1-\sigma(x))$
$$
\color{red}\sigma '(u)= \sigma(u)(1-\sigma(u))\\ \frac{dy}{du}=y(1-y)\\ \frac{du}{dx}=w
$$
得到：
$$
\frac {dy}{dx}= \frac{dy}{du}\frac {du}{dx}=y(1-y)w=\frac{w}{1+e^{-(wx+b)}}[1-\frac{1}{1+e^{-(wx+b)}}]
$$


#### 多变量函数的链式法则

在多变量函数的情况下，链式法则的思想也同样适用。只要像处理分数一样对导数的式子进行变形即可。然而事情并没有这么简单，因为必须对相关的全部变量应用链式法则。

变量 $z$ 为 $u$、$v$ 的函数  $z=f(u,v)$，如果 $u$、$v$  分别为 $x$、$y$ 的函数$u=g(x,y),v=h(x,y)$，则 $z$ 为 $x$、$y$的函数$z=f(g(x,y),h(x,y))$，此时下式（多变量函数的链式法则）成立。
$$
\frac{\partial z}{\partial x}=\frac{\partial z}{\partial u}\frac{\partial u}{\partial x}+\frac{\partial z}{\partial v}\frac{\partial v}{\partial x}\tag{求偏导}
\\
\frac{\partial z}{\partial y}=\frac{\partial z}{\partial u}\frac{\partial u}{\partial y}+\frac{\partial z}{\partial v}\frac{\partial v}{\partial y} 
$$
即：变量 $z$ 为 $u$、$v$ 的函数，$u$、$v$ 分别为 $x$、$y$ 的函数，$z$ 关于 $x$ 求导时，先对 $u$、$v$ 求导，然后与 $z$ 的相应导数相乘，最后将乘积加起来。如图：

<img src="深度学习.assets/image-20210528123829984-1622176712445.png" alt="image-20210528123829984" style="zoom:80%;" />

<img src="深度学习.assets/image-20210528124013000-1622176815124.png" alt="image-20210528124013000" style="zoom:80%;" />

三个以上的多变量函数的情况下也同样成立！



### 九、多变量函数的近似公式（梯度下降的基础）

梯度下降法是确定神经网络的一种代表性的方法。在应用梯度下降法时，需要用到多变量函数的近似公式。

#### 单变量函数的近似公式

已知导数定义：
$$
f′(x) =\lim_{\Delta x\rightarrow 0}\frac{f(x+ \Delta x)-f(x)}{\Delta x}{}
$$
将$\Delta x$这个”无限趋于0“或者说”无限小“的量，替换成”微小”的量，得到：
$$
f′(x) \approx \frac{f(x+ \Delta x)-f(x)}{\Delta x}  \\
$$
整理得：
$$
f(x+ \Delta x)\approx  f(x)+ f′(x){\Delta x}
$$
这个就是，**单变量函数的近似公式**，这里$\Delta x$是"微小得数"

举例：$y=f(x)=e^{x}$ ，求其 $x=0$ 附近 得近似公式
$$
(e^x)'=e^x  \\
e^{x+\Delta x} \approx  e^{x}+e^{x}\Delta x\\
x\rightarrow0\\
e^x\approx 1+\Delta x \approx 1+x
$$
得到：$y=e^x$  和其在$x\rightarrow 0$时得近似函数 $y=1+x$

如图：

<img src="深度学习.assets/image-20210528131611536-1622178972633.png" alt="image-20210528131611536" style="zoom:80%;" />



#### 多变量函数的近似公式

将单变量函数的近似公式扩展到两个变量的函数。如果$x$、$y$ 作微小的变化，那么函数 $z = f(x, y)$ 的值将会怎样变化呢？答案就是**多变量函数近似公式**。*∆x*、*∆y* 为微小的数。
$$
f(x+ \Delta x，y+ \Delta y)\approx  f(x,y)+ \frac{\partial f(x,y)}{\partial x}{\Delta x}+\frac{\partial f(x,y)}{\partial y}{\Delta y}
$$
化简一下：定义$\Delta z =f(x+ \Delta x，y+ \Delta y)-f(x,y) $   得到：
$$
\Delta z \approx \frac{\partial z}{\partial x}\Delta x +\frac{\partial z}{\partial y}\Delta y
$$
通过这样的简化方式，就很容易将近似公式进行推广到哪个变量$x_1,x_2,\cdots ,x_n$。

例如，变量 $z$为四个变量 $w,x,y,b$的函数时，近似公式如下所示
$$
\Delta z \approx \frac{\partial z}{\partial w}\Delta w +\frac{\partial z}{\partial x}\Delta x +\frac{\partial z}{\partial y}\Delta y+\frac{\partial z}{\partial b}\Delta b
$$

#### 近似公式的向量表示

如上，四个变量的函数的近似公式 ，可以表示为如下两个向量的内积：（复习一下向量内积的坐标表示）
$$
梯度向量：
\nabla \alpha =( \frac{\partial z}{\partial w} ,\frac{\partial z}{\partial x},\frac{\partial z}{\partial y},\frac{\partial z}{\partial b})\\

位移向量：\Delta \beta =(\Delta w ,\Delta x ,\Delta y ,\Delta b )\\
$$
对于一般的 *n* 变量函数，近似公式也可以像这样表示为内积的形式



### 十、梯度下降法的含义与公式

应用数学最重要的任务之一就是寻找函数取最小值的点。研究一下著名的寻找最小值的点的方法——梯度下降法

主要通过两个变量的函数来展开。在神经网络的计算中，往往需要处理成千上万个变量，但其数学原理和两个变量的情形是相同的

#### 梯度下降法的思路

1. 已知函数 *z* = *f*(*x*, *y*)，怎样求使函数取得最小值的 *x*、*y* 呢？最有名的方法就是利用“使函数 *z* = *f* (*x*, *y*) 取得最小值的 *x*、*y* 满足以下关系”：

$$
\frac{\partial f(x,y)}{\partial x}=0 \qquad \frac{\partial f(x,y)}{\partial y}=0 
$$

2. 然而，在实际问题中，联立上面的方程式通常不容易求解
3. 梯度下降法是一种具有代表性的替代方法。该方法不直接求解上面的方程，而是通过慢慢地移动图像上的点进行摸索，从而找出函数的最小值。
4. 

#### 向量内积的回顾

两个固定大小的非零向量 $\vec{a}$、$\vec{b}$。当 $\vec{b}$ 的方向与 $\vec{a}$ 相反时，内积 **a** *·* **b** 取最小值

换句话说，当向量  $\vec {b}$   满足 $ \vec{b}=-k\vec{a} $   时，可以使得内积 **a** *·* **b** 取最小值

#### 二变量函数的梯度下降法的基本式

$$
(\Delta x,\Delta y)=- \eta (\frac{\partial f(x,y)}{\partial x},\frac{\partial f(x,y)}{\partial y})
$$

利用关系式 (5)，如果从点(*x*, *y*)向点(*x* + *∆x*, *y* + *∆y*)移动，就可以从图像上点 (*x*, *y*) 的位置最快速地下坡。

定义来了，向量$(\frac{\partial f(x,y)}{\partial x},\frac{\partial f(x,y)}{\partial y})$称为函数$f(x,y)$在点$(x,y)$处的**梯度（gradient)**

举例：设 $∆x、∆y$ 为微小的数。在函数 $z = x^2 + y^2$ 中，当 $x$ 从 1 变到 $1+\Delta x$、$y $  从 2 变到$2+\Delta y$  时，求使这个函数减小得最快的向量 $(∆x, ∆y)$。

解：

有：$(\Delta x,\Delta y)=- \eta (\frac{\partial f(x,y)}{\partial x},\frac{\partial f(x,y)}{\partial y})$

有：$z=f(x,y) = x^2 + y^2$

得：$\frac{\partial f(x,y)}{\partial x}=2x \qquad ,\frac{\partial f(x,y)}{\partial y}=2y$   

有：$(x,y)=(1,2)$ ，从（1，2）出发

使得函数$f(x,y)$减小最快的向量是：$-\eta(2,4)$

#### 梯度下降法及其用法

二变量函数的梯度下降法：

步骤1：利用$(\Delta x,\Delta y)=- \eta (\frac{\partial f(x,y)}{\partial x},\frac{\partial f(x,y)}{\partial y})$找到函数$f(x,y)$减小最快的方向$(\Delta x,\Delta y)$

步骤2：从点 $(x, y)$ 向点  $(x + ∆x, y + ∆y)$  移动 ，到达新的点$(x,y)$

步骤3：重复步骤1和步骤2，反复运算，得到$f(x,y)$的最小值

#### 将梯度下降法推广到三个变量以上的情况

**梯度下降法基本式**：
$$
(\Delta x_1,\Delta x_2,\cdots,\Delta x_n)=- \eta (\frac{\partial nf(x_1,x_2,\cdots,x_n)}{\partial x_1},\frac{\partial f(x_1,x_2,\cdots,x_n)}{\partial x_2},\cdots,\frac{\partial f(x_1,x_2,\cdots,x_n)}{\partial x_n})
$$
注意，左边的向量也称为位移向量，可以简写成$\Delta x$，右边的向量可以简写成：哈密顿算子 $\nabla f$

得到梯度下降法基本式的**简洁写法**：
$$
\Delta x = - \eta  \nabla f
$$
再注意：

#### **η** 的含义以及梯度下降法的要点

*η* 可以看作人移动时的“步长”，根据 *η* 的值，可以确定下一步移动到哪个点。

如果步长较大，那么可能会到达最小值点，也可能会直接跨过了最小值点（左图）。

而如果步长较小，则可能会滞留在极小值点（右图）

<img src="深度学习.assets/image-20210528162501792.png" alt="image-20210528162501792" style="zoom:80%;" />

在神经网络的世界中，*η* 也称为学习率。遗憾的是，它的确定方法没有明确的标准，只能通过反复试验来寻找恰当的值。

说明1：梯度下降法也可以用于单变量函数，只要梯度向量解释为一维向量（*n* = 1）的情况就可以了。也就是说，将偏导数替换为导数，将得到的下式作为梯度下降法的基本式。*∆x* = - *ηf'*(*x*)　（*η* 为正的微小常数）

说明2：将 *η* 看作步长，实际上这并不严谨，正确的说法是，位移向量的大小才是步长。不过，虽然人的步长大体上是固定的，但梯度下降法的“步长”是不均匀的。因为梯度在不同的位置大小不同。因此，在应用数学的数值计算中，有时会对梯度下降法的基本式进行其他变形~~。



### 十一、用Excel体验梯度下降法

<img src="深度学习.assets/image-20210528164517007-1622191518564.png" alt="image-20210528164517007" style="zoom:80%;" />

### 十二、最优化问题和回归分析

从数学上来说，确定神经网络的参数是一个最优化问题，具体就是对神经网络的参数（即权重和偏置）进行拟合，使得神经网络的输出与

实际数据相吻合。

为了理解最优化问题，最浅显的例子就是回归分析。

#### 什么是回归分析

由多个变量组成的数据中，着眼于其中一个特定的变量，用其余的变量来解释这个特定的变量，这样的方法称为回归分析。回归分析的种类有

很多。

最简单的**一元线性回归分析**，以两个变量组成的数据作为考察对象，用一条直线近似地表示右图所示的散点图上的点列，通过该直线的方程来考察两个变量之间的关系。该直线称为**回归直线**

有训练数据如下：

| 身高（x) | 体重（y) |
| -------- | -------- |
| 174.1    | 61.5     |
| $\cdots$ | $\cdots$ |
| 177.8    | 66.1     |

回归方程，x是自变量，y是因变量，模型（model）： $y=wx+b$

训练数据：自变量$x_i$，因变量  $y_i$

预测值：$\hat{y}=wx+b$

预测值和实际测试数据的误差：${y_i}-\hat{y_i}=y_i-(wx_i+b)$

平方误差：$C_i=\frac{1}{2}({y_i}-(wx_i+b)^2)$

平方误差和（损失函数，代价函数）：$loss(x,y)=\sum_{i=1}^n\frac{1}{2}({y_i}-(wx_i+b))^2 $

在这个简单的线性回归中，我们的目标是：找到合适的$w,b$，尽可能让损失函数$loass(x,y)$取最小值0，即：
$$
\frac{\partial loss(x,y)}{\partial w}=0  \qquad \frac{\partial loss(x,y)}{\partial b}=0
$$
根据复合函数链式偏导法则，得到：
$$
\frac{\partial loss(x,y)}{\partial w}= \sum_{i=1}^n  x_i(y_i-(wx_i+b))=0\\
\qquad \frac{\partial loss(x,y)}{\partial b}=\sum_{i=1}^n  -(y_i-(wx_i+b))=0
$$
联立求解$w,b$



综上，机器学习过程中

步骤1：确定了模型（线性回归直线模型中只有两个参数$w,b$），即，根据身高和体重的训练数据去找到预测体重的回归直线方程

步骤2：通过大量训练数据求累加和，使其等于0（或趋近于0），联立求解，得到模型中的参数$w,b$ ，完成机器学习

注意1：当模型中参数增多，要提供更多的训练数据，数据规模

注意2：训练数据的采集和使用前，要预先进行数据标准化和数据清洗

注意3：损失函数有很多种形式，本例中损失函数是平方误差总和（最小二乘法），最小值为0，本例中代价函数关于w和b两个参数求偏导，如果参数量巨大会怎样？

