## 机器学习/深度学习的数学

### 一、函数

#### 一次函数：

##### 一个自变量的情形：

​							$$y=ax+b  $$   

##### 多个自变量的情形：

​							$$ y=ax_1+bx_2+c (a、b、c是常数) $$

##### 神经网络中的一次函数：

​		神经网络中，神经单元的加权输入可以表示为一次函数关系。例如，神经单元有三个来自下层的输入，其加权输入 *z* 的式子如下所示：

​							$$z=w_1x_1+w_2x_2+w_3x_3+b$$

- 如果把作为参数的权重 $w_1,w_2,w_3$与偏置 $b$ 看作常数，那么加权输入 $z$ 和$x_1,x_2,x_3$是一次函数关系。
- 另外，在神经单元的输入 $x_1,x_2,x_3$作为数据值确定了的情况下，加权输入$z$ 和权重 $w_1,w_2,w_3$以及偏置$b$是一次函数关系。
- 用误差反向传播法推导计算式时，这些一次函数关系使得计算可以简单地进行。



#### 二次函数：

##### 机器学习和深度学习中的代价函数使用了二次函数。

二次函数由下式表示：  $y=ax^2+bx+c$

![image-20210527095101484](深度学习.assets/image-20210527095101484-1622080266513.png)这个图像中重要的一点是，$a$ 为正数时图像向下凸，从而存在最小值。

下凸的函数有最小值，这个性质是机器学习中最小二乘法的基础

##### 多个自变量的二次函数：

$$y=ax_1^2+bx_1x_2+cx_2^2+px_1+qx_2+r$$

![image-20210527100116577](深度学习.assets/image-20210527100116577-1622080879219.png)神经网络中要处理的二次函数



#### 单位阶跃函数：

神经网络的原型模型是用单位阶跃函数作为激活函数

$$f(x)=\begin{cases}0, & \text{(x<  0)} \\1, & \text{(x }\geq\text{0)}\end{cases}$$ 此函数在原点不可导，所以不能作为主要的激活函数

![image-20210527104504044](深度学习.assets/image-20210527104504044.png)

#### 指数函数与Sigmoid函数：

##### 指数函数：$$y=a^x     , (a>0,\text{a}\ne\text{1})$$

##### 重要的底：自然数，欧拉数，纳皮尔数 $e=2.71828 \cdots$

##### Sigmoid函数：

$$\sigma(x)=\frac{1}{1+e^{-x}}$$     $\sigma(x)$是神经网络中，代表性的激活函数

<img src="深度学习.assets/image-20210527110037925-1622084440838.png" alt="image-20210527110037925" style="zoom: 67%;" />



#### 正态分布的概率密度函数

用计算机实际确定神经网络时，必须设定权重和偏置的初始值。求初始值时，正态分布（normal

 distribution）是一个有用的工具。使用服从这个分布的随机数，容易取得好的结果。

正态分布是服从以下概率密度函数 *f*(*x*) 的概率分布

$$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^\frac{(x-\mu)^2}{2\sigma^2}$$    其中$\sigma$是标准差，$\mu$ 是期望值（平均值）

<img src="深度学习.assets/image-20210527111046300-1622085048056.png" alt="image-20210527111046300" style="zoom:80%;" />

<img src="深度学习.assets/image-20210527111158868-1622085121545.png" alt="image-20210527111158868" style="zoom:80%;" />

在 Excel 中，可以像下面这样产生正态分布随机数。

$= NORM.INV(RAND(), µ, σ)    \qquad \qquad（µ、σ 是期望值和标准差）$

<img src="深度学习.assets/image-20210527122626831-1622089589464.png" alt="image-20210527122626831" style="zoom:67%;" />



**方差或者说标准差越大，离散程度越大**

参考阅读：https://blog.csdn.net/zhaodedong/article/details/103775963 （关于期望值和平均值）

参考阅读：https://blog.csdn.net/u013066730/article/details/83029619  （方差和标准差）





### 二、数列



理解了数列的递推公式，就很容易理解：求解预估值的正向传播（链式求值）、误差的反向传播（链式求导）

注意：计算机更擅长用递推公式进行运算，联想下程序设计中的递归

#### 数列的通项公式和递推公式

- 等差数列通项公式：$a_n=a_1+(n-1)d$
- 等比数列通项公式：$a_n=a_1q^{n-1}$
- 等差数列递推公式：$a_{n+1}=a_n+d$
- 等比数列递推公式：$a_{n+1}=a_nq$
- 神经网络中，每一个神经单元可以理解为数列中的某一项

#### 联立的递推公式

$$\begin{cases}a_{n+1}=a_n+2b_n+2 \\ b_{n+1}=2a_n+3b_n+3 \end{cases}$$

如上，两个数列的联立递推公式，尝试理解如下神经网络中每个神经单元的值的计算



<img src="深度学习.assets/image-20210527130632915-1622091995702.png" alt="image-20210527130632915" style="zoom:80%;" />

**注：其中，$a_n^m ,b_n^m$    ：第m层第n个神经单元的值和偏置（激活神经单元的阈值） ；**

**注：其中，$w_i^j$ ：  第j层每个结点接收上层对应结点的数据的权重**



$$\begin{cases} a_1^3=\alpha(w_{11}^3a_1^2+w_{12}^3a_2^2+w_{13}^3a_3^2+b_1^3)\\  a_2^3=\alpha(w_{21}^3a_1^2+w_{22}^3a_2^2+w_{23}^3a_3^2+b_2^3) \end{cases}$$



根据这个递推公式，第 3 层的输出 $a_1^3$和$a_2^3$由第 2 层的输出 $a_1^2$、 $a_2^2$ 、$a_3^2$决定。

也就是说，第 2 层的输出与第 3 层的输出由联立递推公式联系起来。



### 三、∑符号的理解和使用

∑是一个需要下功夫来熟悉的符号。如果不理解∑，在阅读神经网络相关的文献时就比较麻烦。

这是因为将加权输入用∑符号来表示会简洁得多。

#### 常见的希腊字母：

| \sigma   | $$\Sigma \text{大写}   \qquad \sigma  \text{小写}$$    |
| -------- | ------------------------------------------------------ |
| \alpha   | $$\Alpha \text{大写}   \qquad \alpha  \text{小写}$$    |
| \beta    | $$\Beta \text{大写}   \qquad \beta \text{小写}$$       |
| \gamma   | $$\Gamma \text{大写}   \qquad \gamma  \text{小写}$$    |
| \delta   | $$\Delta \text{大写}   \qquad \delta \text{小写}$$     |
| \epsilon | $$\Epsilon \text{大写}   \qquad \epsilon \text{小写}$$ |
| \zeta    | $$ \Zeta \text{大写}   \qquad \zeta\text{小写}$$       |
| \eta     | $$\Eta \text{大写}   \qquad \eta \text{小写}$$         |
| \theta   | $$\Theta \text{大写}   \qquad \theta  \text{小写}$$    |
| \lambda  | $$\Lambda \text{大写}   \qquad \lambda  \text{小写}$$  |

#### ∑符号含义：表示数列的总和

$$\sum_{i=1}^n a_i= a_1+a_2+\cdots+a_n$$

#### ∑符号性质：和的∑为∑的和，常数倍的∑为∑的常数倍

$$\sum_{i=1}^n(a_i+b_i)=\sum_{i=1}^na_i+\sum_{i=1}^nb_i$$

$$\sum_{i=1}^nca_i=c\sum_{i=1}^na_i$$

**在机器学习和深度学习的文献中，把加权输入用∑符号不是非常简洁**。





### 四、向量(Vector)

#### 定义：

- 有向线段的属性：起点 *A* 的位置、指向 *B* 的方向，以及*AB* 的长度，也就是大小
- 把方向与大小抽象出来，这样的量叫作**向量**
- 向量是具有方向与大小的量，用箭头表示。
- 有向线段 *AB* 所代表的向量用 $\vec{AB}$ 表示，也可以用带箭头的单个字母$\vec{a}$表示。



#### 向量的坐标表示法

- 把向量的箭头放在坐标系中，就可以用坐标的形式表示向量。
- 把箭头的起点放在原点，用箭头终点的坐标表示向量。
- 用坐标表示的向量 $\vec{a}$，$\vec{a}=(a_1,a_2)$
- 三维空间同理：$\vec{a}=(a_1,a_2,a_3)$  ，$\vec{a}=(1,2,2)$，如图

<img src="深度学习.assets/image-20210527150751221.png" alt="image-20210527150751221" style="zoom:80%;" />

#### 向量的大小： | **a** |

表示向量的箭头的**长度**称为这个向量的大小。向量 $\vec{a}$ 的大小用 | **a** | 表示。

#### 向量的内积：$$\vec{a}\cdot\vec{b} = \left| \vec{a} \right|\left| \vec{b}\right|\cos(\theta) $$

<img src="深度学习.assets/image-20210527152613424-1622100375353.png" alt="image-20210527152613424" style="zoom:67%;" />

#### 柯西 - 施瓦茨不等式：$-\left|a\right|\left|b\right|\le a \cdot  b \le \left|a\right|\left|b\right|$

根据柯西 - 施瓦茨不等式，可以得出以下事实。

- 当两个向量方向相反时，内积取得最小值。**梯度下降法的基本原理**
- 当两个向量不平行时，内积取平行时的中间值。**越大越相似，考察卷积神经网络**
- 当两个向量方向相同时，内积取得最大值

<img src="深度学习.assets/image-20210527154146908-1622101308932.png" alt="image-20210527154146908" style="zoom:67%;" />

#### 内积的坐标表示

当$\vec{a}=(a1,a2)$，$\vec{b}=(b1,b2)$时，

​		$$\vec{a} \cdot \vec{b}= a_1b_1+a_2b_2	$$

同理，三维空间情况下：当$\vec{a}=(a1,a2,a3)$，$$\vec{b}=(b1,b2,b3)$$时，

​		$$\vec{a} \cdot \vec{b}= a_1b_1+a_2b_2+a_3b_3$$

#### 向量的一般化

- 向量的方便之处在于，二维以及三维空间中的性质可以照搬到任意维空间中；
- 神经网络虽然要处理数万维的空间，但是二维以及三维空间的向量性质可以直接利用。基于此，向量被充分应用在梯度下降法中；
- 二维以及三维空间中的向量公式推广到任意的 *n* 维空间：
  - 向量的坐标表示法： $\vec{a}=(a_1,a_2,\cdots,a_n)$
  - 内积的坐标表示：$$\vec{a} \cdot \vec{b}= a_1b_1+a_2b_2+\cdots+a_nb_n$$

- 在神经网络中使用内积形式：
  - <img src="深度学习.assets/image-20210527161141507-1622103103036.png" alt="image-20210527161141507" style="zoom:67%;" />
  - 如图有两个向量：$\vec{x}=(x_1,x_2,\cdots,x_n)$  和 $\vec{w}=(w_1,w_2,\cdots,w_n)$
  - 加权输入可表示为向量内积形式：$z=\vec{x}\cdot\vec{w}+b$



### 五、矩阵(Matrix)

#### 什么是矩阵

- 矩阵就是数的阵列，如下：横排为行，竖排为列，构成一个3行3列的矩阵
- 行数和列数相同，称为**方阵**；
- (1,2,3)、(4,5,6)、(7,8,9)，称为**行向量**；(1,4,7)、(2,5,8)、(3,6,9)称为**列向量**；
- 行向量、列向量，也统称**向量**

$$
\begin{Bmatrix}
   1 & 2 & 3 \\
   4 & 5 & 6 \\
   7 & 8 & 9
  \end{Bmatrix}
$$

- 扩展为一般形式：m行n列的矩阵

$$
\left\{
 \begin{matrix}
 a_{11}      & a_{12}        & \cdots & a_{1n}        \\
 a_{21}      & a_{22}        & \cdots & a_{2n}       \\
 \vdots & \vdots   & \ddots & \vdots   \\
 a_{m1} & a_{m2} & \cdots & a_{mn} 
 \end{matrix}
 \right\}  \tag{m,n的矩阵}
$$

- 位于第 i 行第 j 列的值（称为元素），用$a_{ij}$表示

- 很有名的矩阵（**单位矩阵**）：对角线的元素$(a_{ii})$为1，其他元素为0的**方阵**，用**E**表示

  $$E=\begin{Bmatrix}
     1 &  0 & 0 \\
      0 & 1 &  0 \\
      0 &  0 & 1
    \end{Bmatrix} \tag{注：E 为德语中表示 1 的单词 Ein 的首字母}$$

#### 矩阵相等

​		两个矩阵 **A**、**B** 相等的含义是它们对应的元素相等，记为 **A** *=* **B**

#### 矩阵的和、差、常数倍

​		两个矩阵 **A**、**B** 的和 **A** *+* **B**、差 **A** - **B** 定义为相同位置的元素的和、差所产生的矩阵。此外，矩阵的常数倍定义为各个元素的常数倍所产生的矩阵。

#### 矩阵的乘积

- 矩阵的乘积在神经网络的应用中特别重要。对于两个矩阵 **A**、**B**，将**A** 的第 *i* 行看作行向量，**B** 的第 *j* 列看作列向量，将它们的**内积**作为第 *i*行第 *j* 列元素，由此而产生的矩阵就是矩阵 **A**、**B** 的乘积 $$A \cdot B$$。
- <img src="深度学习.assets/image-20210527204317388-1622119398950.png" alt="image-20210527204317388" style="zoom:80%;" />
- 矩阵的乘法不满足交换律。也就是说，除了例外情况，以下关系式成立：$$A \cdot B\ne B \cdot A$$
- 不是任何两个矩阵都能够相乘，**只有乘数矩阵A的列数和被乘矩阵B的行数相同的时候，两个矩阵才能相乘。**
- 维度为（m x n）的矩阵 A 和维度为（n x p）的矩阵 B 相乘，最终得到维度为（m x p）的矩阵 C。
- <img src="深度学习.assets/image-20210527205751320-1622120272827.png" alt="image-20210527205751320" style="zoom:67%;" />

#### Hadamard 乘积

对于相同形状的矩阵 **A**、**B**，将相同位置的元素相乘，由此产生的矩阵称为矩阵 **A**、**B** 的 **Hadamard** 乘积，用 **A**⊙**B** 表示。

#### 转置矩阵

- 将矩阵 **A** 的第 *i* 行第 *j* 列的元素与第 *j* 行第 *i* 列的元素交换，由此产生的矩阵称为矩阵 **A** 的转置矩阵（Transposed Matrix），用 $ t^A $ 或   $A^T$等表示。
- 可以理解为把行向量转换成列向量（反之亦然，把列向量转换成行向量）

<img src="深度学习.assets/image-20210527210238336-1622120559686.png" alt="image-20210527210238336" style="zoom:67%;" />



#### 标量（Scalar)和张量（Tensor）理解

- 在机器学习和深度学习中，核心的数据结构就是标量、向量、矩阵和张量；
- 其中标量是单个数字，或者说，若 x∈ℝ 表示 x 是一个标量。
- 其中张量是更泛化的实体，是对标量，向量和矩阵的更高层封装，可以理解为，张量的特例是矩阵，矩阵的特例是向量，向量的特例是标量；
- 如下图：多个矩阵，或者说6个矩阵组成的一个张量

<img src="深度学习.assets/image-20210527212438222-1622121879660.png" alt="image-20210527212438222" style="zoom:50%;" />

谷歌提供的人工智能学习系统（TensorFlow)的命名中使用了这个属于，据说来源于物理学中的"tension"(张力)，好比一个物体不同的截面上产生的应力方向和大小都不相同，张量是应力在数学上的抽象。





### 六、导数（derivatives）

#### 导数的定义

$$
f′(x) =\lim_{\Delta x\rightarrow 0}\frac{f(x+ \Delta x)-f(x)}{\Delta x}
$$

导函数的含义如下图所示。作出函数 *f* (*x*) 的图像，*f '*(*x*) 表示图像切线的斜率。因此，具有光滑图像的函数是可导的。

![image-20210527215447322](深度学习.assets/image-20210527215447322.png)

导函数的含义。$f'(x)$ 表示图像切线的斜率。实际上，如果 *Q* 无限接近 *P*（也就是∆*x* → 0），那么直线 *PQ* 无限接近切线 *l*。

#### 神经网络中用到的函数的导数公式

我们很少使用上面的定义式来求导函数，而是使用导数公式。

下面我们就来看一下在神经网络的计算中使用的函数的导数公式（*x* 为变量、*c* 为常数）。


$$
(c)'=0 \qquad\qquad  (e)'=0   \tag{常数的导数为0}\\
(x)'=1 \\
(x^2)'= 2x \\
(e^x)' = e^x \\
(e^{-x})' = -e^{-x}
$$

#### 导数符号

函数 *y* = *f* (*x*) 的导函数用 *f '*(*x*) 表示，但也存在不同的表示方法，比如可以用分数形式来表示：$$f'(x)=\frac{dy}{dx}   $$

这个表示方法是十分方便的，这是因为复杂的函数可以像分数一样计算导数



#### 导数的性质

线性性：和的导数为导数的和，常数倍的导数为导数的常数倍。
$$
[f(x)+g(x)]'=f'(x)+g'(x) \\
[cf(x)]' = cf'(x)
$$
导数的线性性是误差反向传播法背后的主角（损失函数loss的反向传播）



#### 分数函数的导数和 Sigmoid 函数的导数

当函数是分数形式时，求导时可以使用下面的分数函数的求导公式：
$$
[\frac{1}{f(x)}]'=-\frac{f(x)}{[f(x)]^2}
$$
Sigmoid 函数 *σ*(*x*) 是神经网络中最有名的激活函数之一：$\sigma(x)=\frac{1}{1+e^{-x}}$

将  $1+e^{-x}$ 代入上式：
$$
\sigma '(x)=- \frac{(1+e^{-x})'}{(1+e^{-x})^2}=\frac{e^{-x}}{(1+e^{-x})^2} =\frac{1+e^{-x}-1}{(1+e^{-x})^2}=\frac{1}{1+e^{-x}}-\frac{1}{(1+e^{-x})^2}
=\sigma(x)-\sigma(x)^2
$$


**提取$\sigma(x)$ ，得到： $ \color{red}\sigma '(x)= \sigma(x)(1-\sigma(x))$** 

梯度下降法中，需要对这个Sigmoid函数求导。求导时使用上式会十分方便。



#### 最小值的条件

由于导函数 *f'*(*x*) 表示切线斜率，我们可以得到以下原理：**当函数*f*(*x*)在*x* = *a*处取得最小值时，*f'*(*a*) = 0**。 

<img src="深度学习.assets/image-20210527225333876-1622127215402.png" alt="image-20210527225333876"  />

或者换句话说：***f'*(*a*) = 0 是函数 *f* (*x*)  在*x* = *a*   处取得最小值的必要条件**

**但是**，看看如下例子：

![image-20210527225701451](深度学习.assets/image-20210527225701451-1622127422982.png)

此函数，虽然是连续光滑的，但有不只一处增减性的变化，所以，**f'*(*a*) = 0 是函数 *f* (*x*)  在*x* = *a*   处取得最小值的必要条件，仅仅是必要条件**

总结：需要连续光滑的**可导函数**，还要求是**凸函数**







### 七、神经网络的偏导数基础

