## 机器学习/深度学习的数学

### 一、函数1

#### 一次函数：

##### 一个自变量的情形：

​							$$y=ax+b  $$   

##### 多个自变量的情形：

​							$$ y=ax_1+bx_2+c (a、b、c是常数) $$

##### 神经网络中的一次函数：

​		神经网络中，神经单元的加权输入可以表示为一次函数关系。例如，神经单元有三个来自下层的输入，其加权输入 *z* 的式子如下所示：

​							$$z=w_1x_1+w_2x_2+w_3x_3+b$$

- 如果把作为参数的权重 $w_1,w_2,w_3$与偏置 $b$ 看作常数，那么加权输入 $z$ 和$x_1,x_2,x_3$是一次函数关系。
- 另外，在神经单元的输入 $x_1,x_2,x_3$作为数据值确定了的情况下，加权输入$z$ 和权重 $w_1,w_2,w_3$以及偏置$b$是一次函数关系。
- 用误差反向传播法推导计算式时，这些一次函数关系使得计算可以简单地进行。



#### 二次函数：

##### 机器学习和深度学习中的代价函数使用了二次函数。

二次函数由下式表示：  $y=ax^2+bx+c$

![image-20210527095101484](深度学习.assets/image-20210527095101484-1622080266513.png)这个图像中重要的一点是，$a$ 为正数时图像向下凸，从而存在最小值。

下凸的函数有最小值，这个性质是机器学习中最小二乘法的基础

##### 多个自变量的二次函数：

$$y=ax_1^2+bx_1x_2+cx_2^2+px_1+qx_2+r$$

![image-20210527100116577](深度学习.assets/image-20210527100116577-1622080879219.png)神经网络中要处理的二次函数



#### 单位阶跃函数：

神经网络的原型模型是用单位阶跃函数作为激活函数

$$f(x)=\begin{cases}0, & \text{(x<  0)} \\1, & \text{(x }\geq\text{0)}\end{cases}$$ 此函数在原点不可导，所以不能作为主要的激活函数

![image-20210527104504044](深度学习.assets/image-20210527104504044.png)

#### 指数函数与Sigmoid函数：

##### 指数函数：$$y=a^x     , (a>0,\text{a}\ne\text{1})$$

##### 重要的底：自然数，欧拉数，纳皮尔数 $e=2.71828 \cdots$

##### Sigmoid函数：

$$\sigma(x)=\frac{1}{1+e^{-x}}$$     $\sigma(x)$是神经网络中，代表性的激活函数

<img src="深度学习.assets/image-20210527110037925-1622084440838.png" alt="image-20210527110037925" style="zoom: 67%;" />



#### 正态分布的概率密度函数

用计算机实际确定神经网络时，必须设定权重和偏置的初始值。求初始值时，正态分布（normal

 distribution）是一个有用的工具。使用服从这个分布的随机数，容易取得好的结果。

正态分布是服从以下概率密度函数 *f*(*x*) 的概率分布

$$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^\frac{(x-\mu)^2}{2\sigma^2}$$    其中$\sigma$是标准差，$\mu$ 是期望值（平均值）

<img src="深度学习.assets/image-20210527111046300-1622085048056.png" alt="image-20210527111046300" style="zoom:80%;" />

<img src="深度学习.assets/image-20210527111158868-1622085121545.png" alt="image-20210527111158868" style="zoom:80%;" />

在 Excel 中，可以像下面这样产生正态分布随机数。

$= NORM.INV(RAND(), µ, σ)    \qquad \qquad（µ、σ 是期望值和标准差）$

<img src="深度学习.assets/image-20210527122626831-1622089589464.png" alt="image-20210527122626831" style="zoom:67%;" />



**方差或者说标准差越大，离散程度越大**

参考阅读：https://blog.csdn.net/zhaodedong/article/details/103775963 （关于期望值和平均值）

参考阅读：https://blog.csdn.net/u013066730/article/details/83029619  （方差和标准差）





### 二、数列



理解了数列的递推公式，就很容易理解：求解预估值的正向传播（链式求值）、误差的反向传播（链式求导）

注意：计算机更擅长用递推公式进行运算，联想下程序设计中的递归

#### 数列的通项公式和递推公式

- 等差数列通项公式：$a_n=a_1+(n-1)d$
- 等比数列通项公式：$a_n=a_1q^{n-1}$
- 等差数列递推公式：$a_{n+1}=a_n+d$
- 等比数列递推公式：$a_{n+1}=a_nq$
- 神经网络中，每一个神经单元可以理解为数列中的某一项

#### 联立的递推公式

$$\begin{cases}a_{n+1}=a_n+2b_n+2 \\ b_{n+1}=2a_n+3b_n+3 \end{cases}$$

如上，两个数列的联立递推公式，尝试理解如下神经网络中每个神经单元的值的计算



<img src="深度学习.assets/image-20210527130632915-1622091995702.png" alt="image-20210527130632915" style="zoom:80%;" />

**注：其中，$a_n^m ,b_n^m$    ：第m层第n个神经单元的值和偏置（激活神经单元的阈值） ；**

**注：其中，$w_i^j$ ：  第j层每个结点接收上层对应结点的数据的权重**



$$\begin{cases} a_1^3=\alpha(w_{11}^3a_1^2+w_{12}^3a_2^2+w_{13}^3a_3^2+b_1^3)\\  a_2^3=\alpha(w_{21}^3a_1^2+w_{22}^3a_2^2+w_{23}^3a_3^2+b_2^3) \end{cases}$$



根据这个递推公式，第 3 层的输出 $a_1^3$和$a_2^3$由第 2 层的输出 $a_1^2$、 $a_2^2$ 、$a_3^2$决定。

也就是说，第 2 层的输出与第 3 层的输出由联立递推公式联系起来。



### 三、∑符号的理解和使用

∑是一个需要下功夫来熟悉的符号。如果不理解∑，在阅读神经网络相关的文献时就比较麻烦。

这是因为将加权输入用∑符号来表示会简洁得多。

#### 常见的希腊字母：

| \sigma   | $$\Sigma \text{大写}   \qquad \sigma  \text{小写}$$    |
| -------- | ------------------------------------------------------ |
| \alpha   | $$\Alpha \text{大写}   \qquad \alpha  \text{小写}$$    |
| \beta    | $$\Beta \text{大写}   \qquad \beta \text{小写}$$       |
| \gamma   | $$\Gamma \text{大写}   \qquad \gamma  \text{小写}$$    |
| \delta   | $$\Delta \text{大写}   \qquad \delta \text{小写}$$     |
| \epsilon | $$\Epsilon \text{大写}   \qquad \epsilon \text{小写}$$ |
| \zeta    | $$ \Zeta \text{大写}   \qquad \zeta\text{小写}$$       |
| \eta     | $$\Eta \text{大写}   \qquad \eta \text{小写}$$         |
| \theta   | $$\Theta \text{大写}   \qquad \theta  \text{小写}$$    |
| \lambda  | $$\Lambda \text{大写}   \qquad \lambda  \text{小写}$$  |

#### ∑符号含义：表示数列的总和

$$\sum_{i=1}^n a_i= a_1+a_2+\cdots+a_n$$

#### ∑符号性质：和的∑为∑的和，常数倍的∑为∑的常数倍

$$\sum_{i=1}^n(a_i+b_i)=\sum_{i=1}^na_i+\sum_{i=1}^nb_i$$

$$\sum_{i=1}^nca_i=c\sum_{i=1}^na_i$$

**在机器学习和深度学习的文献中，把加权输入用∑符号不是非常简洁**。





### 四、向量

#### 定义：

- 有向线段的属性：起点 *A* 的位置、指向 *B* 的方向，以及*AB* 的长度，也就是大小
- 把方向与大小抽象出来，这样的量叫作**向量**
- 向量是具有方向与大小的量，用箭头表示。
- 有向线段 *AB* 所代表的向量用 $\vec{AB}$ 表示，也可以用带箭头的单个字母$\vec{a}$表示。



#### 向量的坐标表示法

- 把向量的箭头放在坐标系中，就可以用坐标的形式表示向量。
- 把箭头的起点放在原点，用箭头终点的坐标表示向量。
- 用坐标表示的向量 $\vec{a}$，$\vec{a}=(a_1,a_2)$
- 三维空间同理：$\vec{a}=(a_1,a_2,a_3)$  ，$\vec{a}=(1,2,2)$，如图

<img src="深度学习.assets/image-20210527150751221.png" alt="image-20210527150751221" style="zoom:80%;" />

#### 向量的大小： | **a** |

表示向量的箭头的**长度**称为这个向量的大小。向量 $\vec{a}$ 的大小用 | **a** | 表示。

#### 向量的内积：$$\vec{a}\cdot\vec{b} = \left| \vec{a} \right|\left| \vec{b}\right|\cos(\theta) $$

<img src="深度学习.assets/image-20210527152613424-1622100375353.png" alt="image-20210527152613424" style="zoom:67%;" />

#### 柯西 - 施瓦茨不等式：$-\left|a\right|\left|b\right|\le a \cdot  b \le \left|a\right|\left|b\right|$

根据柯西 - 施瓦茨不等式，可以得出以下事实。

- 当两个向量方向相反时，内积取得最小值。**梯度下降法的基本原理**
- 当两个向量不平行时，内积取平行时的中间值。**越大越相似，考察卷积神经网络**
- 当两个向量方向相同时，内积取得最大值

<img src="深度学习.assets/image-20210527154146908-1622101308932.png" alt="image-20210527154146908" style="zoom:67%;" />

#### 内积的坐标表示

当$\vec{a}=(a1,a2)$，$\vec{b}=(b1,b2)$时，

​		$$\vec{a} \cdot \vec{b}= a_1b_1+a_2b_2	$$

同理，三维空间情况下：当$\vec{a}=(a1,a2,a3)$，$$\vec{b}=(b1,b2,b3)$$时，

​		$$\vec{a} \cdot \vec{b}= a_1b_1+a_2b_2+a_3b_3$$

#### 向量的一般化

- 向量的方便之处在于，二维以及三维空间中的性质可以照搬到任意维空间中；
- 神经网络虽然要处理数万维的空间，但是二维以及三维空间的向量性质可以直接利用。基于此，向量被充分应用在梯度下降法中；
- 二维以及三维空间中的向量公式推广到任意的 *n* 维空间：
  - 向量的坐标表示法： $\vec{a}=(a_1,a_2,\cdots,a_n)$
  - 内积的坐标表示：$$\vec{a} \cdot \vec{b}= a_1b_1+a_2b_2+\cdots+a_nb_n$$

- 在神经网络中使用内积形式：
  - <img src="深度学习.assets/image-20210527161141507-1622103103036.png" alt="image-20210527161141507" style="zoom:67%;" />
  - 如图有两个向量：$\vec{x}=(x_1,x_2,\cdots,x_n)$  和 $\vec{w}=(w_1,w_2,\cdots,w_n)$
  - 加权输入可表示为向量内积形式：$z=\vec{x}\cdot\vec{w}+b$



### 五、矩阵

#### 什么是矩阵

- 矩阵就是数的阵列，如下：横排为行，竖排为列，构成一个3行3列的矩阵
- 行数和列数相同，称为**方阵**；
- (1,2,3)、(4,5,6)、(7,8,9)，称为**行向量**；(1,4,7)、(2,5,8)、(3,6,9)称为**列向量**；
- 行向量、列向量，也统称**向量**

$$
\begin{Bmatrix}
   1 & 2 & 3 \\
   4 & 5 & 6 \\
   7 & 8 & 9
  \end{Bmatrix}
$$

- 扩展为一般形式：m行n列的矩阵

$$
\left\{
 \begin{matrix}
 a_{11}      & a_{12}        & \cdots & a_{1n}        \\
 a_{21}      & a_{22}        & \cdots & a_{2n}       \\
 \vdots & \vdots   & \ddots & \vdots   \\
 a_{m1} & a_{m2} & \cdots & a_{mn} 
 \end{matrix}
 \right\}
$$

- 位于第 i 行第 j 列的值（称为元素），用$a_{ij}$表示

- 很有名的矩阵（**单位矩阵**）：对角线的元素$(a_{ii})$为1，其他元素为0的**方阵**，用**E**表示



#### 矩阵相等

#### 矩阵的和、差、常数倍

#### 矩阵的乘积

​		矩阵的乘积在神经网络的应用中特别重要。对于两个矩阵 **A**、**B**，将**A** 的第 *i* 行看作行向量，**B** 的第 *j* 列看作列向量，将它们的内积作为第 *i*行第 *j* 列元素，由此而产生的矩阵就是矩阵 **A**、**B** 的乘积 **AB**。

#### Hadamard 乘积

对于相同形状的矩阵 **A**、**B**，将相同位置的元素相乘，由此产生的矩阵称为矩阵 **A**、**B** 的 **Hadamard** 乘积，用 **A**⊙**B** 表示。

#### 转置矩阵

将矩阵 **A** 的第 *i* 行第 *j* 列的元素与第 *j* 行第 *i* 列的元素交换，由此产生的矩阵称为矩阵 **A** 的转置矩阵（transposed matrix），用 t**A**、**A**t 等表示。